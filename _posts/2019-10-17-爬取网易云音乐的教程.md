---
layout:     post                                               # 使用的布局（不需要改）
title:      来爬个虫 | 教你如何爬取 网易云音乐 的评论                    # 标题 
subtitle:   最佳乐评我知道                                 # 副标题
date:       2019-10-17                                        # 时间
author:     BenYourKing                                       # 作者
header-img: img/office-1.jpg                             # 这篇文章标题背景图片
catalog: true                                                 # 是否归档
tags:                                                         # 标签
    - Tutorial
    - 爬虫
---


> The bug forces the software to apdate ,evolve into something new because of it,               
> work around it or work through it .                   
> No matter what ，it changes ，it become something new，the next version，the inevitable upgrade.              


### 引言 

来爬个虫，本期我们的目标是爬取**网易云音乐歌曲的评论信息**，            

这些评论信息对我们有什么用呢？             
1. 首先，我们可以知道**每首歌的评论数**，以此来模糊判断歌曲的**受欢迎程度**；
2. 其次，我们可以知道**歌曲的评论信息**，**热评**，里面你会发现大家各个都是人才； 
3. 最后，我们可以知道**用户的评论行为**，如果你进一步抓取用户ID，你可以知道ta的**听歌、歌单、个人信息等**。

![](https://ftp.bmp.ovh/imgs/2019/10/627feb3babdf51aa.png)

### 免责声明

为了避免网易云音乐给我寄送律师函警告，首先当然是要来一个免责声明。本博客代码内容参考了[music163crawler](https://github.com/RitterHou/music-163)的代码，如有侵犯网易云音乐公司以及用户的权利，请联系本人进行及时删除。      
   
本帖仅供技术探讨和交流，请不要用做商业和盈利用途。顺便说一句，我的手机里只有[网易云音乐](https://music.163.com)这款APP，希望网易云音乐越来越好！   


### 爬虫逻辑
            
> 古人云：知己知彼，百战百胜。     
> 尼古拉斯·本拉钦云：来爬个虫，先看网页！      
        
             
第①步，先打开，网页版的[网易云音乐](https://music.163.com)                         
我们的目标是**爬取网易云音乐所有歌曲下面的热门评论和最新评论**（*假设就爬20条，或者全部，你开心就好！*）。           
那么我们首先需要了解网易云音乐的架构，根据这个架构确定爬虫的逻辑。          
            
#### URL规律

我们知道，当我们要大规模爬取一个网站上的信息的时候，我们首要确定**URL变化的规律**，才能够用循环语句遍历所有的网页。           
                
以[李荣浩的《年少有为》](https://music.163.com/#/song?id=1293886117)为例，网址为`https://music.163.com/#/song?id=1293886117` ,不难发现，每首歌曲的网页网址为`https://music.163.com/#/song?id=` +`歌曲ID`,这里的歌曲ID，就类似于每个人的身份证号一样。
                
那么我们**如何知道所有全部歌曲的ID呢？**             
        
#### 爬虫逻辑
        
> 凡你在网页上看到的内容    
> 你都能通过爬虫抓取下来     
        
        
爬虫策略：每一个歌手，有很多张专辑；每一张专辑，有很多首歌；每一首歌，下面有很多条评论。     

##### 获取歌手ID

在歌手列表界面，[网易云音乐歌手列表](https://music.163.com/#/discover/artist)，我们可以发现，每一个**歌手**都在一个对应的**组别**下面，每一个**组别**，按照**字母顺序A~Z**以及**其他**，分门别类的安放。`例如` ， 李荣浩，在**华语男歌手**，字母**L**里面。         
![](https://ftp.bmp.ovh/imgs/2019/10/05dce08288f2e6cc.png)
            
在这个歌手列表的界面，我们需要的**信息就在网页源代码里**`鼠标移动到任一歌手下面---鼠标右键--检查`，我们可以爬取到 **全部歌手** 这一数据！          
看以上图片，我们知道**歌手列表的URL循环有三部分构成**：     
        
1. `基础URL`:保持不变的部分。`https://music.163.com/#/discover/artist/cat?`;      
2. `组别`：第一个参数 `id = [1001,1002,1003,2001,2002,2003，···，]`;          
3. `字母顺序`：第二个参数`initial = [65,66,67,...,85,87,88,89,90,0]`.         

我们获得了包括李荣浩在内的所有歌手的ID，李荣浩在网易云音乐的ID为4292。

#### 获取专辑ID

在歌手个人主页，我们可以抓取歌手发布的专辑的信息**专辑ID**+**专辑名称**等；         
个人主页中所有专辑列表的URL为`https://music.163.com/#/artist/album?id=`+`我们上一步得到的每个歌手的ID`，例如李荣浩歌手ID为4292，那么他的专辑列表为，`https://music.163.com/#/artist/album?id=4292`                

![](https://ftp.bmp.ovh/imgs/2019/10/6c78037f3f2d3599.png)

在这一个界面下，我们需要的**信息就在网页源代码里**`鼠标移动到任一歌手下面---鼠标右键--检查`，我们可以爬取到 **某一位歌手的全部专辑** 这一数据！            
![](https://ftp.bmp.ovh/imgs/2019/10/0669eeb59364d6f6.png)                 

这里我们发现，**歌手有不止一页的专辑，就涉及到翻页的问题**，怎么办呢？这里有一个小技巧。              
以李荣浩为例，他                
第一页专辑的URL为：`https://music.163.com/#/artist/album?id=4292`           
第二页的专辑URL为： `https://music.163.com/#/artist/album?id=4292&limit=12&offset=12`           
我们看到第二页比第一页多了一些东西，`limit`表示每一页限制显示多少个专辑，`offset`为下一页相比于第一页的偏移量（也就是前面有多少专辑）.一种简单粗暴的方法就是，**直接将limit设置成200**，使所有专辑显示在一页里面。         
                 
#### 获取歌曲ID

得到专辑ID后，我们就可以来到专辑的页面了。              
某一位歌手某一张专辑的URL为`https://music.163.com/#/album?id=`+`我们上一步得到的每张专辑的ID`，例如李荣浩的专辑`耳朵`ID为73914415，那么这张专辑歌曲列表的URL为：`https://music.163.com/#/album?id=73914415`。      
                
![](https://ftp.bmp.ovh/imgs/2019/10/eceea728148eb197.png)          
                
在这一个界面下，我们需要的**信息就在网页源代码里**`鼠标移动到任一歌手下面---鼠标右键--检查`，我们可以爬取到 **某一张专辑全部歌曲ID** 这一数据！    











     

















































































